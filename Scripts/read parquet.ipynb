{
	"metadata": {
		"kernelspec": {
			"name": "glue_pyspark",
			"display_name": "Glue PySpark",
			"language": "python"
		},
		"language_info": {
			"name": "Python_Glue_Session",
			"mimetype": "text/x-python",
			"codemirror_mode": {
				"name": "python",
				"version": 3
			},
			"pygments_lexer": "python3",
			"file_extension": ".py"
		}
	},
	"nbformat_minor": 4,
	"nbformat": 4,
	"cells": [
		{
			"cell_type": "markdown",
			"source": "# AWS Glue Studio Notebook\n##### You are now running a AWS Glue Studio notebook; To start using your notebook you need to start an AWS Glue Interactive Session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "markdown",
			"source": "#### Optional: Run this cell to see available notebook commands (\"magics\").\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "%help",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": null,
			"outputs": []
		},
		{
			"cell_type": "markdown",
			"source": "####  Run this cell to set up and start your interactive session.\n",
			"metadata": {
				"editable": true,
				"trusted": true
			}
		},
		{
			"cell_type": "code",
			"source": "import sys\nfrom awsglue.transforms import *\nfrom awsglue.utils import getResolvedOptions\nfrom pyspark.context import SparkContext\nfrom awsglue.context import GlueContext\nfrom awsglue.job import Job\n  \nsc = SparkContext.getOrCreate()\nglueContext = GlueContext(sc)\nspark = glueContext.spark_session\njob = Job(glueContext)",
			"metadata": {
				"trusted": true,
				"editable": true
			},
			"execution_count": 1,
			"outputs": [
				{
					"name": "stdout",
					"text": "Welcome to the Glue Interactive Sessions Kernel\nFor more information on available magic commands, please type %help in any new cell.\n\nPlease view our Getting Started page to access the most up-to-date information on the Interactive Sessions kernel: https://docs.aws.amazon.com/glue/latest/dg/interactive-sessions.html\nInstalled kernel version: 1.0.5 \nTrying to create a Glue session for the kernel.\nSession Type: glueetl\nSession ID: e4e17e9d-58d2-4c67-bd8c-b4d46ff02255\nApplying the following default arguments:\n--glue_kernel_version 1.0.5\n--enable-glue-datacatalog true\nWaiting for session e4e17e9d-58d2-4c67-bd8c-b4d46ff02255 to get into ready status...\nSession e4e17e9d-58d2-4c67-bd8c-b4d46ff02255 has been created.\n\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "import sys\nfrom awsglue.context import GlueContext\nfrom pyspark.context import SparkContext\nfrom awsglue.utils import getResolvedOptions\n\n# Initialize Glue context\nglueContext = GlueContext(SparkContext.getOrCreate())\n\n# Define S3 paths\ninput_path = \"s3://etherscandata/raw_data/invoke-ethereum_transactions.json\"\noutput_path = \"s3://etherscandata/new-parquet/\"\n\n# Step 1: Read the JSON file from S3\ndynamic_frame = glueContext.create_dynamic_frame.from_options(\n    connection_type=\"s3\", \n    format=\"json\", \n    connection_options={\"paths\": [input_path]}\n)\n\n# Step 2: Convert to DataFrame for easier manipulation (optional)\ndataframe = dynamic_frame.toDF()\n\n# Step 3: Write the DataFrame as Parquet back to S3\ndataframe.write.parquet(output_path)\n\nprint(\"Conversion to Parquet complete!\")\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 3,
			"outputs": [
				{
					"name": "stdout",
					"text": "Conversion to Parquet complete!\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Correct path based on your input\nparquet_file_path = \"s3://etherscandata/new-parquet/part-00002-55005096-6b24-4d05-9d1a-3c98ddfd466f-c000.snappy.parquet\"\n\n# Step 1: Read the parquet file into a DataFrame\ndf = spark.read.parquet(parquet_file_path)\n\n# Step 2: Show the data (limit the rows by using the 'limit' function)\ndf.show(10)  # Show the first 10 rows\n\n# Optionally, display the schema to understand the structure of the data\ndf.printSchema()\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 7,
			"outputs": [
				{
					"name": "stdout",
					"text": "+--------------------+--------------------+--------------------+-----------+--------------------+----------------+--------------------+--------+-------+-------+----+------+\n|             address|              topics|                data|blockNumber|     transactionHash|transactionIndex|           blockHash|logIndex|removed|jsonrpc|  id|result|\n+--------------------+--------------------+--------------------+-----------+--------------------+----------------+--------------------+--------+-------+-------+----+------+\n|0x386b76d9ca5f5fb...|[0x2849b43074093a...|0x000000000000000...|  0x13c09a9|0xbc6c9bbd41648d0...|            0x4f|0x1e1710ec6677b13...|    0xdc|  false|   null|null|  null|\n|0x17bfafa932d2e23...|[0xb3813568d9991f...|0x000000000000000...|  0x13c09a9|0xbc6c9bbd41648d0...|            0x4f|0x1e1710ec6677b13...|    0xdd|  false|   null|null|  null|\n|0x17bfafa932d2e23...|[0xcfcb62499f5737...|0x000000000000000...|  0x13c09a9|0xbc6c9bbd41648d0...|            0x4f|0x1e1710ec6677b13...|    0xde|  false|   null|null|  null|\n|0x2a721cbe81a128b...|[0xcb0f7ffd78f9ae...|0x000000000000000...|  0x13c09a9|0xbc6c9bbd41648d0...|            0x4f|0x1e1710ec6677b13...|    0xdf|  false|   null|null|  null|\n|0x2a721cbe81a128b...|[0x8ebb2ec2465bdb...|0x000000000000000...|  0x13c09a9|0xbc6c9bbd41648d0...|            0x4f|0x1e1710ec6677b13...|    0xe0|  false|   null|null|  null|\n|0xb4e16d0168e52d3...|[0x8c5be1e5ebec7d...|0x000000000000000...|  0x13c09a9|0x5562f8e36786360...|            0x51|0x1e1710ec6677b13...|    0xe1|  false|   null|null|  null|\n|0xb4e16d0168e52d3...|[0xddf252ad1be2c8...|0x000000000000000...|  0x13c09a9|0x5562f8e36786360...|            0x51|0x1e1710ec6677b13...|    0xe2|  false|   null|null|  null|\n|0xb4e16d0168e52d3...|[0xddf252ad1be2c8...|0x000000000000000...|  0x13c09a9|0x5562f8e36786360...|            0x51|0x1e1710ec6677b13...|    0xe3|  false|   null|null|  null|\n|0xa0b86991c6218b3...|[0xddf252ad1be2c8...|0x000000000000000...|  0x13c09a9|0x5562f8e36786360...|            0x51|0x1e1710ec6677b13...|    0xe4|  false|   null|null|  null|\n|0xc02aaa39b223fe8...|[0xddf252ad1be2c8...|0x000000000000000...|  0x13c09a9|0x5562f8e36786360...|            0x51|0x1e1710ec6677b13...|    0xe5|  false|   null|null|  null|\n+--------------------+--------------------+--------------------+-----------+--------------------+----------------+--------------------+--------+-------+-------+----+------+\nonly showing top 10 rows\n\nroot\n |-- address: string (nullable = true)\n |-- topics: array (nullable = true)\n |    |-- element: string (containsNull = true)\n |-- data: string (nullable = true)\n |-- blockNumber: string (nullable = true)\n |-- transactionHash: string (nullable = true)\n |-- transactionIndex: string (nullable = true)\n |-- blockHash: string (nullable = true)\n |-- logIndex: string (nullable = true)\n |-- removed: boolean (nullable = true)\n |-- jsonrpc: string (nullable = true)\n |-- id: integer (nullable = true)\n |-- result: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- address: string (nullable = true)\n |    |    |-- topics: array (nullable = true)\n |    |    |    |-- element: string (containsNull = true)\n |    |    |-- data: string (nullable = true)\n |    |    |-- blockNumber: string (nullable = true)\n |    |    |-- transactionHash: string (nullable = true)\n |    |    |-- transactionIndex: string (nullable = true)\n |    |    |-- blockHash: string (nullable = true)\n |    |    |-- logIndex: string (nullable = true)\n |    |    |-- removed: boolean (nullable = true)\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "# Step 1: Read the parquet file into a DataFrame\nparquet_file_path = \"s3://etherscandata/new-parquet/part-00002-55005096-6b24-4d05-9d1a-3c98ddfd466f-c000.snappy.parquet\"\ndf = spark.read.parquet(parquet_file_path)\n\n# Step 2: Check for columns with NULL values\nnull_columns = []\nfor column in df.columns:\n    null_count = df.filter(df[column].isNull()).count()\n    if null_count > 0:\n        null_columns.append((column, null_count))\n\n# Step 3: Print the columns with their corresponding NULL counts\nif len(null_columns) > 0:\n    print(\"Columns with NULL values and their counts:\")\n    for col, count in null_columns:\n        print(f\"Column: {col}, NULL count: {count}\")\nelse:\n    print(\"No columns contain NULL values.\")\n",
			"metadata": {
				"trusted": true,
				"tags": []
			},
			"execution_count": 9,
			"outputs": [
				{
					"name": "stdout",
					"text": "Columns with NULL values and their counts:\nColumn: jsonrpc, NULL count: 79665\nColumn: id, NULL count: 79665\nColumn: result, NULL count: 79665\n",
					"output_type": "stream"
				}
			]
		},
		{
			"cell_type": "code",
			"source": "",
			"metadata": {},
			"execution_count": null,
			"outputs": []
		}
	]
}